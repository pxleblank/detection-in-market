1) Для работы в реальном времени над задачей детекции выбрал архитектуру YOLO (работает быстро, пропуская пропозишнл боксы и считая сразу анхор бокс) и соответствующий фреймворк.


### Инструкция:
- Чтобы начать обучение, вставьте путь к обучающему и размеченному (частично) видео в переменную VIDEO_PATH в первой ячейке файла main.ipynb (также в папке должна находится разметка в виде annotations.xml) и запустите её.
- Уже обученная модель лежит в runs/final_train/weights/best.pt
- Ячейкой ниже можно проверить лейбы на картинке.
- В файле events_test.ipynb можно проверить ивенты, которые были в ТЗ, перед этим надо настроить конфиг бд в DATABASE_CONFIG и в process_video("") указать путь к видео.
- Во view_test.ipynb можно создать видео и посмотреть как модель детектит объекты. В переменной VIDEO_PATH также нужно указать путь к исходному видео.
- В test_data_market.csv лежат результаты теста детекции ивентов и в папке event кадры к ним.

### Описание:
Думал, что одно видео для тренировки, другое для тест, поэтому модель не сильно хорошо обучилась, к тому же разметил только треть первых кадров и решил использовать semi-supervised learning для создания псевдо-лейблов, поэтому не учлись палеты, например, без целлофана.

### В runs лежат результаты обучения. 


Надо было выбрать более легкий трекер без реиндефекации (ByteTrack) и оптимизировать модель. + ещё добавить такую проверку на закрытие паллета навесом (закрывает ли его сначала человек, а потом навес)